{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa41bcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready.\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "import os, glob, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Paths ----\n",
    "BASE_DIR      = \"../data/refs\"\n",
    "PTFE_DIR      = os.path.join(BASE_DIR, \"ptfe_1000\")  # 1000 CSV for pure PTFE\n",
    "HBN_DIR       = os.path.join(BASE_DIR, \"hbn_thin_1000\")   # 1000 CSV for pure hBN\n",
    "OUT_DIR       = \"../generated/bulk_50k\"               # OUTPUT FOLDER (NPZ shards)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Film physics \n",
    "t_ptfe_um = 200.0    # PTFE film thickness (µm) \n",
    "t_hbn_um  = 540.0   # hBN \"reference\" thickness (µm)  (updated for the thin pellet)\n",
    "rho_ptfe  = 2.20     # g/cm^3\n",
    "rho_hbn   = 0.3     # g/cm^3  (updated for the thin pellet)\n",
    "\n",
    "# When mixing, we “record” on a PTFE-like film of this thickness:\n",
    "t_mix_um  = t_ptfe_um\n",
    "\n",
    "# ---- Dataset settings ----\n",
    "N_SYNTH          = 50_000           # total synthetic samples to generate\n",
    "HBN_MIN, HBN_MAX = 0, 50            # inclusive range for random hBN %\n",
    "RANDOM_SEED      = 42               # reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ---- Saving (shards) ----\n",
    "SHARD_SIZE = 5_000                  # 50k -> 10 shards of 5k; adjust if needed\n",
    "assert N_SYNTH % SHARD_SIZE == 0, \"For simplicity, set N_SYNTH multiple of SHARD_SIZE\"\n",
    "META_JSON = os.path.join(OUT_DIR, \"meta.json\")\n",
    "\n",
    "print(\"Config ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b31da5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils ready.\n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "from typing import List, Tuple\n",
    "\n",
    "def build_file_list(folder: str) -> List[str]:\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"*.CSV\")))\n",
    "    if not files:\n",
    "        files = sorted(glob.glob(os.path.join(folder, \"*.csv\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {folder}\")\n",
    "    return files\n",
    "\n",
    "def load_xy_any(fn: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load 2-col CSV (header/no-header tolerant) -> (x, y) as float arrays.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(fn, header=0)\n",
    "        x = pd.to_numeric(df.iloc[:,0], errors=\"coerce\").to_numpy()\n",
    "        y = pd.to_numeric(df.iloc[:,1], errors=\"coerce\").to_numpy()\n",
    "        if np.isnan(x).any() or np.isnan(y).any(): raise ValueError\n",
    "    except Exception:\n",
    "        df = pd.read_csv(fn, header=None)\n",
    "        x = pd.to_numeric(df.iloc[:,0], errors=\"coerce\").to_numpy()\n",
    "        y = pd.to_numeric(df.iloc[:,1], errors=\"coerce\").to_numpy()\n",
    "        mask = ~(np.isnan(x) | np.isnan(y))\n",
    "        x, y = x[mask], y[mask]\n",
    "    return x, y\n",
    "\n",
    "print(\"Utils ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79a5f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PTFE: 1000 files | hBN: 1000 files\n",
      "Interpolated to common grid; wn points: 3323\n"
     ]
    }
   ],
   "source": [
    "# Load raw, find common wn grid, interp\n",
    "ptfe_files = build_file_list(PTFE_DIR)\n",
    "hbn_files  = build_file_list(HBN_DIR)\n",
    "print(f\"Found PTFE: {len(ptfe_files)} files | hBN: {len(hbn_files)} files\")\n",
    "\n",
    "# Load all x,y once to find overlap\n",
    "raw_ptfe = [load_xy_any(f) for f in ptfe_files]\n",
    "raw_hbn  = [load_xy_any(f) for f in hbn_files]\n",
    "\n",
    "# Determine safe/common wavenumber window\n",
    "wn_min = max(min(x.min() for x,_ in raw_ptfe),\n",
    "             min(x.min() for x,_ in raw_hbn))\n",
    "wn_max = min(max(x.max() for x,_ in raw_ptfe),\n",
    "             max(x.max() for x,_ in raw_hbn))\n",
    "\n",
    "assert wn_max - wn_min > 500, \"Common spectral window too small; check raw files.\"\n",
    "\n",
    "# 1 cm^-1 spacing\n",
    "wn = np.arange(math.ceil(wn_min), math.floor(wn_max)+1, 1.0)\n",
    "\n",
    "def make_bank(raw_list, file_list):\n",
    "    bank = []\n",
    "    for (x,y), fn in zip(raw_list, file_list):\n",
    "        yi = np.interp(wn, x, y)\n",
    "        bank.append({\"file\": os.path.basename(fn), \"A_raw\": yi})\n",
    "    return bank\n",
    "\n",
    "ptfe_bank = make_bank(raw_ptfe, ptfe_files)\n",
    "hbn_bank  = make_bank(raw_hbn,  hbn_files)\n",
    "print(\"Interpolated to common grid; wn points:\", len(wn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c33b439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank harmonized & normalized.\n"
     ]
    }
   ],
   "source": [
    "# LSQ-scale to canonical medians; areal-mass norm\n",
    "\n",
    "def lsq_scale_to_ref(A, ref):\n",
    "    \"\"\"Return scalar s that minimizes ||s*A - ref||_2.\"\"\"\n",
    "    denom = float(np.dot(A, A))\n",
    "    return float(np.dot(A, ref) / denom) if denom > 0 else 1.0\n",
    "\n",
    "# Build initial group medians\n",
    "ptfe_stack = np.vstack([d[\"A_raw\"] for d in ptfe_bank])\n",
    "hbn_stack  = np.vstack([d[\"A_raw\"] for d in hbn_bank])\n",
    "ptfe_med   = np.median(ptfe_stack, axis=0)\n",
    "hbn_med    = np.median(hbn_stack,  axis=0)\n",
    "\n",
    "# Scale each to median\n",
    "for d in ptfe_bank:\n",
    "    s = lsq_scale_to_ref(d[\"A_raw\"], ptfe_med)\n",
    "    d[\"scale_to_med\"] = s\n",
    "    d[\"A_scaled\"]     = s * d[\"A_raw\"]\n",
    "\n",
    "for d in hbn_bank:\n",
    "    s = lsq_scale_to_ref(d[\"A_raw\"], hbn_med)\n",
    "    d[\"scale_to_med\"] = s\n",
    "    d[\"A_scaled\"]     = s * d[\"A_raw\"]\n",
    "\n",
    "# Recompute canonical medians after scaling (optional refinement)\n",
    "ptfe_stack2 = np.vstack([d[\"A_scaled\"] for d in ptfe_bank])\n",
    "hbn_stack2  = np.vstack([d[\"A_scaled\"] for d in hbn_bank])\n",
    "ptfe_ref    = np.median(ptfe_stack2, axis=0)\n",
    "hbn_ref     = np.median(hbn_stack2,  axis=0)\n",
    "\n",
    "# Areal-mass normalization\n",
    "um_to_cm  = 1e-4\n",
    "t_ptfe_cm = t_ptfe_um * um_to_cm\n",
    "t_hbn_cm  = t_hbn_um  * um_to_cm\n",
    "t_mix_cm  = t_mix_um  * um_to_cm\n",
    "\n",
    "def to_areal_mass_norm(A_scaled, rho, t_cm):\n",
    "    return A_scaled / (rho * t_cm)\n",
    "\n",
    "for d in ptfe_bank:\n",
    "    d[\"A_norm\"] = to_areal_mass_norm(d[\"A_scaled\"], rho_ptfe, t_ptfe_cm)\n",
    "\n",
    "for d in hbn_bank:\n",
    "    d[\"A_norm\"] = to_areal_mass_norm(d[\"A_scaled\"], rho_hbn, t_hbn_cm)\n",
    "\n",
    "print(\"Bank harmonized & normalized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cbbfd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mix helper ready.\n"
     ]
    }
   ],
   "source": [
    "# Mixing helper (PTFE + hBN, %)\n",
    "def mix_spectrum(ptfe_percent, A_ptfe_norm, A_hbn_norm, rho_ptfe, rho_hbn, t_mix_cm):\n",
    "    \"\"\"\n",
    "    Returns A_out (absorbance on film t_mix_cm) and rho_mix.\n",
    "    ptfe_percent in [0,100]; hBN% = 100 - PTFE%.\n",
    "    \"\"\"\n",
    "    w_hbn  = (100.0 - ptfe_percent) / 100.0\n",
    "    w_ptfe = 1.0 - w_hbn\n",
    "    A_mix_norm = w_ptfe * A_ptfe_norm + w_hbn * A_hbn_norm\n",
    "    rho_mix = 1.0 / (w_ptfe / rho_ptfe + w_hbn / rho_hbn)\n",
    "    A_out   = A_mix_norm * (rho_mix * t_mix_cm)\n",
    "    return A_out, float(rho_mix)\n",
    "\n",
    "print(\"Mix helper ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70f1871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic sampler (random pairs & ratios)\n",
    "N_PTFE = len(ptfe_bank)\n",
    "N_HBN  = len(hbn_bank)\n",
    "assert N_PTFE >= 1000 and N_HBN >= 1000, \"Expecting ~1000 CSVs each per your new setup.\"\n",
    "\n",
    "# Pre-extract normalized arrays for speed\n",
    "PTFE_NORM = np.vstack([d[\"A_norm\"] for d in ptfe_bank])  # shape: (N_PTFE, n_wn)\n",
    "HBN_NORM  = np.vstack([d[\"A_norm\"]  for d in hbn_bank])  # shape: (N_HBN , n_wn)\n",
    "\n",
    "def sample_batch(batch_size: int, hbn_min=0, hbn_max=50):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X : (batch_size, n_wn) spectra\n",
    "      y_hbn_pct : (batch_size,) integer hBN %\n",
    "      y_ptfe_pct: (batch_size,) integer PTFE %\n",
    "    \"\"\"\n",
    "    # Randomly choose source indices (with replacement for diversity)\n",
    "    ptfe_idx = np.random.randint(0, N_PTFE, size=batch_size)\n",
    "    hbn_idx  = np.random.randint(0, N_HBN,  size=batch_size)\n",
    "\n",
    "    # Random hBN% in [hbn_min, hbn_max]\n",
    "    y_hbn_pct = np.random.uniform(hbn_min, hbn_max, size=batch_size).astype(np.float32)\n",
    "    y_ptfe_pct = (100 - y_hbn_pct).astype(np.int16)\n",
    "\n",
    "    # Gather source spectra\n",
    "    A_ptfe = PTFE_NORM[ptfe_idx]   # (B, n_wn)\n",
    "    A_hbn  = HBN_NORM[hbn_idx]     # (B, n_wn)\n",
    "\n",
    "    # Mix per-sample\n",
    "    w_hbn  = (y_hbn_pct / 100.0).reshape(-1, 1)\n",
    "    w_ptfe = 1.0 - w_hbn\n",
    "\n",
    "    # rho_mix per sample\n",
    "    rho_mix = 1.0 / (w_ptfe / rho_ptfe + w_hbn / rho_hbn)  # (B,1)\n",
    "    A_mix_norm = w_ptfe * A_ptfe + w_hbn * A_hbn           # (B, n_wn)\n",
    "    X = A_mix_norm * (rho_mix * t_mix_cm)                  # (B, n_wn)\n",
    "\n",
    "    return X.astype(np.float32), y_hbn_pct, y_ptfe_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf921b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved meta → ../generated/bulk_50k/meta.json\n",
      "[1/10] Saved: ../generated/bulk_50k/shard_000.npz\n",
      "[2/10] Saved: ../generated/bulk_50k/shard_001.npz\n",
      "[3/10] Saved: ../generated/bulk_50k/shard_002.npz\n",
      "[4/10] Saved: ../generated/bulk_50k/shard_003.npz\n",
      "[5/10] Saved: ../generated/bulk_50k/shard_004.npz\n",
      "[6/10] Saved: ../generated/bulk_50k/shard_005.npz\n",
      "[7/10] Saved: ../generated/bulk_50k/shard_006.npz\n",
      "[8/10] Saved: ../generated/bulk_50k/shard_007.npz\n",
      "[9/10] Saved: ../generated/bulk_50k/shard_008.npz\n",
      "[10/10] Saved: ../generated/bulk_50k/shard_009.npz\n",
      "All shards saved to: ../generated/bulk_50k\n"
     ]
    }
   ],
   "source": [
    "# Generate N_SYNTH samples and save as compressed NPZ\n",
    "n_points = len(wn)\n",
    "n_shards = N_SYNTH // SHARD_SIZE\n",
    "\n",
    "# Save small metadata json\n",
    "meta = {\n",
    "    \"description\": \"Synthetic IR mixtures for MLP training (PTFE+hBN)\",\n",
    "    \"n_samples\": N_SYNTH,\n",
    "    \"n_shards\": n_shards,\n",
    "    \"shard_size\": SHARD_SIZE,\n",
    "    \"wavenumber_min\": float(wn.min()),\n",
    "    \"wavenumber_max\": float(wn.max()),\n",
    "    \"wavenumber_step\": float(wn[1]-wn[0]) if len(wn) > 1 else None,\n",
    "    \"n_points\": int(n_points),\n",
    "    \"t_ptfe_um\": float(t_ptfe_um),\n",
    "    \"t_hbn_um\": float(t_hbn_um),\n",
    "    \"t_mix_um\": float(t_mix_um),\n",
    "    \"rho_ptfe\": float(rho_ptfe),\n",
    "    \"rho_hbn\": float(rho_hbn),\n",
    "    \"hbn_percent_range\": [int(HBN_MIN), int(HBN_MAX)],\n",
    "    \"random_seed\": int(RANDOM_SEED),\n",
    "}\n",
    "with open(META_JSON, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(f\"Saved meta → {META_JSON}\")\n",
    "\n",
    "# Create shards\n",
    "for s in range(n_shards):\n",
    "    X, y_hbn, y_ptfe = sample_batch(SHARD_SIZE, HBN_MIN, HBN_MAX)\n",
    "    shard_path = os.path.join(OUT_DIR, f\"shard_{s:03d}.npz\")\n",
    "    # Use numpy's compressed format\n",
    "    np.savez_compressed(\n",
    "        shard_path,\n",
    "        X=X,                 # float32, shape (SHARD_SIZE, n_points)\n",
    "        y_hbn=y_hbn,         # int16,   shape (SHARD_SIZE,)\n",
    "        y_ptfe=y_ptfe,       # int16,   shape (SHARD_SIZE,)\n",
    "        wn=wn.astype(np.float32)  # store axis once per shard for convenience\n",
    "    )\n",
    "    print(f\"[{s+1}/{n_shards}] Saved:\", shard_path)\n",
    "\n",
    "print(\"All shards saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d270ebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['X', 'y_hbn', 'y_ptfe', 'wn']\n",
      "Shapes: (5000, 3323) (5000,) (3323,)\n",
      "Example hBN%: [10.073196  13.2604265  8.48796    4.782035  20.956808   5.6649804\n",
      "  8.595628  27.366234   8.229963   3.1939883]\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: load shard 0\n",
    "test_npz = os.path.join(OUT_DIR, \"shard_000.npz\")\n",
    "dat = np.load(test_npz)\n",
    "print(\"Keys:\", list(dat.keys()))\n",
    "print(\"Shapes:\", dat[\"X\"].shape, dat[\"y_hbn\"].shape, dat[\"wn\"].shape)\n",
    "print(\"Example hBN%:\", dat[\"y_hbn\"][:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
